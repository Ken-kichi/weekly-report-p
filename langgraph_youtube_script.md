# オープニング
今回はLangGraphを使って、Gitのログから自動で週報を作成するワークフローを構築します。
CLIでコマンドを打つだけでレポートが完成するので、毎週の報告作業を大幅に短縮できます。
単体のAIに任せると途中の判断が見えにくくなりますが、LangGraphなら各エージェントの状態を追跡でき、条件分岐や繰り返し改善も自在です。
さらに3つの評価AIを導入し、常に80点以上の品質でレポートを仕上げます。
状態が見える化されると、どこで点数が落ちたのかを即座に特定できるので、初心者でも改善サイクルを回しやすくなるのがこの実装方針を選んだ理由です。
リポジトリを指定→ログを要約→評価→改善→最終レポート提示という流れを頭に入れつつ、セットアップへ進みましょう。

# セットアップ
デスクトップに新しいプロジェクトフォルダを作成し、`cd`で移動します。
`uv init`や`git init`でプロジェクトを初期化し、`uv venv`で仮想環境を用意して有効化します。
`uv`を採用しているのは、依存のロックから実行までを同じコマンドで管理でき、LangGraphのような依存が多いプロジェクトでも再現性が高いからです。
ベアなファイル群（`cli.py`や`state.py`など）を空の状態で作り、エディターを起動します。
依存ライブラリ（`langgraph`, `typer`, `rich`など）を`uv add`でインストールし、`.env`にはOpenAI APIキーを記入します。
`.gitignore`に`.env`と`.venv`を追加することで、秘密情報の漏洩と環境差分の混入を防ぎます。

# CLI UXの実装
`cli.py`ではTyperを使って`generate`コマンドを作ります。
Typerを採用するのは、型ヒントから自動生成されるヘルプがLangGraph初心者に優しく、Click互換で将来のサブコマンド拡張もしやすいからです。
引数には解析したいリポジトリパスを受け取り、内部でLangGraphフローを呼び出すだけの薄いレイヤーにすることで、CLIと業務ロジックの結合度を下げています。
リポジトリの存在チェックやログ出力先の案内をCLI側で済ませると、グラフ内のノードは成功フローだけに集中でき、デバッグがシンプルになります。

# 状態管理の設計
`state.py`にはLangGraphで共有する状態クラスを定義します。
Gitログのサマリー、ドラフトレポート、各評価AIのスコア、改善指示、最終レポートなどを一つにまとめ、どのノードからでも参照・更新できるようにします。
単一のStateモデルに集約しているのは、LangGraphのチェックポイント機能と組み合わせて途中結果を巻き戻したり、改善ループの何周目かを即座に追跡するためです。
状態遷移が明示されることで、ボトルネックとなったスコアや未入力のフィールドをその場で確認できます。

# git読み取り処理を実装
`git_loader.py`では指定リポジトリに対して`git log`を呼び出し、期間やコミット数を絞ってサマリーを生成します。
CLIから受け取ったパスを即座に検証しておくことで、LangGraph側に無効パスが流れ込まないようにしています。
コミットメッセージや変更ファイルをまとめたテキストをLangGraphの次のノードに渡し、AIがコンテキストを理解できるよう整形します。
ここで不要なノイズを除去しておくと、後段の生成ノードのトークン消費を抑え、コストを予測しやすくなります。

# 生成処理の実装
`generator.py`ではGitサマリーと過去のレポート方針を入力に、週報のドラフトを作成します。
アウトライン（今週やったこと、成果、課題、来週の予定）を固定しているのは、評価側が毎回同じ枠で採点できるようにするためです。
LangGraphのノードごとに責務を分け、ここでは「一次生成のみ」に専念させることで、改善指示ロジックを独立してチューニングできるようにしています。

# 評価処理の実装
`evaluator.py`ではドラフトに対して観点別の採点を行うエージェントを定義します。
例えば「事実性」「網羅性」「読みやすさ」を個別スコアで返し、改善ポイントのコメントも付けます。
観点を分けているのは、スコアが足りない部分だけを条件分岐で改善ループに回すためで、どの観点が弱いかをStateに記録しておくと次の世代のプロンプトを調整しやすくなります。


# 複数評価処理の実装
`multi_evaluator.py`で3つの評価AIを並列実行し、平均点と最低点を集計します。
複数の評価結果を束ねる中間ノードを用意したのは、LangGraphの分岐とマージを視覚的に整理し、後から評価AIの数を増減しやすくするためです。
合計スコアが80点未満なら改善フローに戻し、十分なら最終化フラグを立てます。
閾値判定をここに集約すると、CLIや生成ノード側に条件分岐が散らばらず、責務分離が保てます。

# 生成フローを構築
`build_graph.py`では各ノード（ローダー、ジェネレーター、評価、改善ループ）をLangGraphのワークフローとして接続します。
依存関係をこの1ファイルに閉じ込めておくと、別のプロジェクトで同じノードを再利用する際にも差し替えポイントが明確になります。
ループ回数や終了条件をStateのフィールドに紐づけるのは、LangGraphの可視化ツールで数値をそのまま確認できるようにし、改善回数の上限を動的に調整する余地を残すためです。

# 実行エントリ
`main.py`でCLIとLangGraphフローを組み合わせ、`uv run main.py generate --repo ~/Desktop/ai-report-p`を実行すれば一連の処理が動きます。
エントリポイントを分けているのは、LangGraphのワークフローをライブラリ的に再利用したいときにCLIを介さず呼び出せるようにするためです。
ログからレポート生成、評価と改善、最終出力までが自動化され、完成した週報をそのまま提出できる状態になります。

# エンディング
AIで業務を自動化するには、今回のようにフロー全体を設計しておくことが不可欠です。
フローがあることで業務が可視化され、品質保証や改善ループ、さらにはルーティンワークの自動化まで一気通貫で実現できます。
ぜひ自分の業務を整理するところから始めて、LangGraphでAIエージェントを実装するところまで挑戦してみてください。
